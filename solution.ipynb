{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d570b256-fef7-44fe-8784-ed9e356688ce",
   "metadata": {},
   "source": [
    "# Object tracking using yolov9 and ByteTrack tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27115c3b-b999-4e54-ba5d-67e5b0accc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c8b190-d3fb-4d95-addc-9cf0eb7a7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1/1 (frame 1/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 20 persons, 4 cars, 2 buss, 9 traffic lights, 1 backpack, 2 handbags, 427.3ms\n",
      "video 1/1 (frame 2/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 18 persons, 2 cars, 1 bus, 6 traffic lights, 2 backpacks, 2 handbags, 414.6ms\n",
      "video 1/1 (frame 3/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 19 persons, 3 cars, 2 buss, 8 traffic lights, 1 backpack, 2 handbags, 379.3ms\n",
      "video 1/1 (frame 4/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 18 persons, 2 cars, 1 bus, 7 traffic lights, 1 backpack, 1 handbag, 371.5ms\n",
      "video 1/1 (frame 5/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 17 persons, 3 cars, 1 bus, 5 traffic lights, 1 backpack, 1 handbag, 387.2ms\n",
      "video 1/1 (frame 6/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 15 persons, 5 cars, 1 bus, 6 traffic lights, 1 handbag, 390.6ms\n",
      "video 1/1 (frame 7/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 17 persons, 3 cars, 2 buss, 5 traffic lights, 1 handbag, 397.4ms\n",
      "video 1/1 (frame 8/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 2 buss, 7 traffic lights, 409.9ms\n",
      "video 1/1 (frame 9/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 3 cars, 2 buss, 7 traffic lights, 416.8ms\n",
      "video 1/1 (frame 10/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 3 cars, 2 buss, 7 traffic lights, 1 backpack, 4 handbags, 394.4ms\n",
      "video 1/1 (frame 11/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 18 persons, 3 cars, 1 bus, 7 traffic lights, 1 handbag, 410.1ms\n",
      "video 1/1 (frame 12/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 19 persons, 4 cars, 2 buss, 6 traffic lights, 2 backpacks, 2 handbags, 417.2ms\n",
      "video 1/1 (frame 13/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 16 persons, 3 cars, 2 buss, 7 traffic lights, 4 backpacks, 1 handbag, 427.3ms\n",
      "video 1/1 (frame 14/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 19 persons, 2 cars, 1 bus, 6 traffic lights, 3 backpacks, 1 handbag, 401.1ms\n",
      "video 1/1 (frame 15/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 18 persons, 2 cars, 2 buss, 7 traffic lights, 372.5ms\n",
      "video 1/1 (frame 16/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 16 persons, 1 car, 2 buss, 5 traffic lights, 3 backpacks, 1 handbag, 1 skateboard, 410.1ms\n",
      "video 1/1 (frame 17/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 15 persons, 1 car, 2 buss, 7 traffic lights, 2 backpacks, 1 handbag, 389.1ms\n",
      "video 1/1 (frame 18/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 19 persons, 1 car, 2 buss, 7 traffic lights, 3 backpacks, 1 handbag, 402.9ms\n",
      "video 1/1 (frame 19/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 16 persons, 1 car, 2 buss, 5 traffic lights, 2 backpacks, 394.2ms\n",
      "video 1/1 (frame 20/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 16 persons, 1 car, 1 bus, 8 traffic lights, 3 backpacks, 394.1ms\n",
      "video 1/1 (frame 21/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 3 cars, 2 buss, 7 traffic lights, 4 backpacks, 402.8ms\n",
      "video 1/1 (frame 22/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 2 cars, 2 buss, 6 traffic lights, 2 backpacks, 1 handbag, 393.1ms\n",
      "video 1/1 (frame 23/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 2 cars, 1 bus, 3 traffic lights, 1 backpack, 1 handbag, 395.1ms\n",
      "video 1/1 (frame 24/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 15 persons, 4 cars, 1 bus, 5 traffic lights, 1 backpack, 387.2ms\n",
      "video 1/1 (frame 25/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 17 persons, 4 cars, 2 buss, 4 traffic lights, 2 backpacks, 396.3ms\n",
      "video 1/1 (frame 26/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 1 bus, 5 traffic lights, 3 backpacks, 3 handbags, 387.7ms\n",
      "video 1/1 (frame 27/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 18 persons, 5 cars, 2 buss, 4 traffic lights, 1 backpack, 1 handbag, 385.9ms\n",
      "video 1/1 (frame 28/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 15 persons, 3 cars, 2 buss, 4 traffic lights, 4 backpacks, 1 handbag, 398.2ms\n",
      "video 1/1 (frame 29/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 3 cars, 2 buss, 4 traffic lights, 2 backpacks, 411.0ms\n",
      "video 1/1 (frame 30/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 4 cars, 1 bus, 5 traffic lights, 4 backpacks, 2 handbags, 383.4ms\n",
      "video 1/1 (frame 31/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 4 cars, 2 buss, 1 truck, 4 traffic lights, 5 backpacks, 432.0ms\n",
      "video 1/1 (frame 32/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 3 backpacks, 4 handbags, 409.1ms\n",
      "video 1/1 (frame 33/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 2 cars, 2 buss, 5 traffic lights, 3 backpacks, 3 handbags, 392.5ms\n",
      "video 1/1 (frame 34/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 15 persons, 2 cars, 2 buss, 5 traffic lights, 4 backpacks, 3 handbags, 400.1ms\n",
      "video 1/1 (frame 35/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 2 cars, 2 buss, 5 traffic lights, 3 backpacks, 1 handbag, 394.3ms\n",
      "video 1/1 (frame 36/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 2 cars, 2 buss, 3 traffic lights, 3 backpacks, 2 handbags, 394.7ms\n",
      "video 1/1 (frame 37/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 3 cars, 1 bus, 4 traffic lights, 3 backpacks, 2 handbags, 387.7ms\n",
      "video 1/1 (frame 38/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 1 bus, 5 traffic lights, 4 backpacks, 1 handbag, 383.3ms\n",
      "video 1/1 (frame 39/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 2 buss, 4 traffic lights, 3 backpacks, 383.7ms\n",
      "video 1/1 (frame 40/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 4 cars, 1 bus, 5 traffic lights, 4 backpacks, 1 handbag, 381.7ms\n",
      "video 1/1 (frame 41/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 2 cars, 2 buss, 4 traffic lights, 2 backpacks, 1 handbag, 387.4ms\n",
      "video 1/1 (frame 42/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 4 cars, 2 buss, 4 traffic lights, 4 backpacks, 400.5ms\n",
      "video 1/1 (frame 43/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 2 cars, 2 buss, 4 traffic lights, 2 backpacks, 401.2ms\n",
      "video 1/1 (frame 44/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 3 cars, 1 bus, 1 truck, 6 traffic lights, 3 backpacks, 409.2ms\n",
      "video 1/1 (frame 45/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 2 cars, 2 buss, 5 traffic lights, 2 backpacks, 384.4ms\n",
      "video 1/1 (frame 46/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 3 cars, 2 buss, 4 traffic lights, 3 backpacks, 398.2ms\n",
      "video 1/1 (frame 47/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 3 backpacks, 1 handbag, 1 cell phone, 388.5ms\n",
      "video 1/1 (frame 48/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 5 cars, 1 bus, 1 truck, 5 traffic lights, 2 backpacks, 1 handbag, 395.3ms\n",
      "video 1/1 (frame 49/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 4 cars, 2 buss, 1 truck, 4 traffic lights, 2 backpacks, 1 handbag, 461.2ms\n",
      "video 1/1 (frame 50/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 1 backpack, 1 handbag, 1 cell phone, 402.2ms\n",
      "video 1/1 (frame 51/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 4 cars, 1 bus, 1 truck, 4 traffic lights, 425.7ms\n",
      "video 1/1 (frame 52/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 432.0ms\n",
      "video 1/1 (frame 53/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 5 persons, 3 cars, 1 bus, 1 truck, 5 traffic lights, 1 backpack, 430.8ms\n",
      "video 1/1 (frame 54/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 2 cars, 2 buss, 4 traffic lights, 1 backpack, 406.0ms\n",
      "video 1/1 (frame 55/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 3 cars, 2 buss, 4 traffic lights, 402.1ms\n",
      "video 1/1 (frame 56/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 4 cars, 1 bus, 3 traffic lights, 2 backpacks, 416.7ms\n",
      "video 1/1 (frame 57/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 4 cars, 1 bus, 4 traffic lights, 1 backpack, 413.2ms\n",
      "video 1/1 (frame 58/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 2 cars, 2 buss, 4 traffic lights, 1 backpack, 414.0ms\n",
      "video 1/1 (frame 59/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 5 cars, 2 buss, 1 truck, 4 traffic lights, 1 backpack, 401.4ms\n",
      "video 1/1 (frame 60/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 4 cars, 1 bus, 1 truck, 4 traffic lights, 425.0ms\n",
      "video 1/1 (frame 61/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 390.9ms\n",
      "video 1/1 (frame 62/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 398.6ms\n",
      "video 1/1 (frame 63/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 395.3ms\n",
      "video 1/1 (frame 64/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 3 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 413.9ms\n",
      "video 1/1 (frame 65/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 4 persons, 3 cars, 2 buss, 1 truck, 4 traffic lights, 416.4ms\n",
      "video 1/1 (frame 66/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 5 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 377.2ms\n",
      "video 1/1 (frame 67/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 4 cars, 1 truck, 5 traffic lights, 402.1ms\n",
      "video 1/1 (frame 68/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 4 cars, 1 bus, 1 truck, 4 traffic lights, 392.9ms\n",
      "video 1/1 (frame 69/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 2 buss, 1 truck, 5 traffic lights, 376.6ms\n",
      "video 1/1 (frame 70/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 4 cars, 2 buss, 4 traffic lights, 1 backpack, 412.0ms\n",
      "video 1/1 (frame 71/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 3 cars, 2 buss, 4 traffic lights, 437.7ms\n",
      "video 1/1 (frame 72/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 3 cars, 2 buss, 4 traffic lights, 391.2ms\n",
      "video 1/1 (frame 73/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 2 buss, 1 truck, 3 traffic lights, 431.7ms\n",
      "video 1/1 (frame 74/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 3 cars, 2 buss, 3 traffic lights, 447.1ms\n",
      "video 1/1 (frame 75/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 4 cars, 2 buss, 4 traffic lights, 1 fire hydrant, 413.3ms\n",
      "video 1/1 (frame 76/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 2 cars, 2 buss, 3 traffic lights, 401.6ms\n",
      "video 1/1 (frame 77/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 3 cars, 1 bus, 3 traffic lights, 385.4ms\n",
      "video 1/1 (frame 78/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 4 cars, 2 buss, 3 traffic lights, 380.9ms\n",
      "video 1/1 (frame 79/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 2 cars, 2 buss, 3 traffic lights, 378.1ms\n",
      "video 1/1 (frame 80/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 2 buss, 3 traffic lights, 371.2ms\n",
      "video 1/1 (frame 81/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 5 persons, 3 cars, 2 buss, 4 traffic lights, 380.7ms\n",
      "video 1/1 (frame 82/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 2 buss, 4 traffic lights, 385.8ms\n",
      "video 1/1 (frame 83/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 382.4ms\n",
      "video 1/1 (frame 84/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 2 cars, 2 buss, 1 truck, 3 traffic lights, 376.3ms\n",
      "video 1/1 (frame 85/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 374.4ms\n",
      "video 1/1 (frame 86/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 383.0ms\n",
      "video 1/1 (frame 87/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 2 cars, 2 buss, 3 traffic lights, 384.7ms\n",
      "video 1/1 (frame 88/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 4 cars, 2 buss, 3 traffic lights, 1 fire hydrant, 385.9ms\n",
      "video 1/1 (frame 89/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 2 buss, 3 traffic lights, 400.1ms\n",
      "video 1/1 (frame 90/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 3 cars, 2 buss, 3 traffic lights, 400.1ms\n",
      "video 1/1 (frame 91/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 2 cars, 2 buss, 3 traffic lights, 396.8ms\n",
      "video 1/1 (frame 92/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 395.5ms\n",
      "video 1/1 (frame 93/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 403.8ms\n",
      "video 1/1 (frame 94/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 413.5ms\n",
      "video 1/1 (frame 95/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 391.3ms\n",
      "video 1/1 (frame 96/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 390.7ms\n",
      "video 1/1 (frame 97/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 3 cars, 2 buss, 1 truck, 3 traffic lights, 392.1ms\n",
      "video 1/1 (frame 98/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 6 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 387.3ms\n",
      "video 1/1 (frame 99/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 402.6ms\n",
      "video 1/1 (frame 100/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 3 cars, 2 buss, 1 truck, 3 traffic lights, 371.3ms\n",
      "video 1/1 (frame 101/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 393.3ms\n",
      "video 1/1 (frame 102/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 377.2ms\n",
      "video 1/1 (frame 103/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 4 cars, 1 bus, 1 truck, 4 traffic lights, 434.1ms\n",
      "video 1/1 (frame 104/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 388.6ms\n",
      "video 1/1 (frame 105/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 399.7ms\n",
      "video 1/1 (frame 106/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 379.8ms\n",
      "video 1/1 (frame 107/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 6 cars, 1 truck, 3 traffic lights, 386.1ms\n",
      "video 1/1 (frame 108/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 377.5ms\n",
      "video 1/1 (frame 109/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 377.6ms\n",
      "video 1/1 (frame 110/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 4 cars, 2 buss, 1 truck, 3 traffic lights, 385.6ms\n",
      "video 1/1 (frame 111/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 378.9ms\n",
      "video 1/1 (frame 112/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 3 cars, 2 buss, 4 traffic lights, 398.3ms\n",
      "video 1/1 (frame 113/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 2 cars, 2 buss, 1 truck, 3 traffic lights, 385.2ms\n",
      "video 1/1 (frame 114/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 3 cars, 2 buss, 3 traffic lights, 388.1ms\n",
      "video 1/1 (frame 115/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 5 cars, 2 buss, 1 truck, 3 traffic lights, 372.8ms\n",
      "video 1/1 (frame 116/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 1 bus, 1 truck, 5 traffic lights, 407.3ms\n",
      "video 1/1 (frame 117/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 388.1ms\n",
      "video 1/1 (frame 118/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 1 skateboard, 1 book, 381.0ms\n",
      "video 1/1 (frame 119/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 373.6ms\n",
      "video 1/1 (frame 120/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 2 buss, 1 truck, 3 traffic lights, 388.0ms\n",
      "video 1/1 (frame 121/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 4 cars, 1 bus, 1 truck, 6 traffic lights, 405.6ms\n",
      "video 1/1 (frame 122/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 377.5ms\n",
      "video 1/1 (frame 123/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 412.6ms\n",
      "video 1/1 (frame 124/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 376.9ms\n",
      "video 1/1 (frame 125/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 385.8ms\n",
      "video 1/1 (frame 126/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 368.7ms\n",
      "video 1/1 (frame 127/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 1 bus, 4 traffic lights, 1 backpack, 378.2ms\n",
      "video 1/1 (frame 128/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 7 cars, 1 bus, 3 traffic lights, 382.5ms\n",
      "video 1/1 (frame 129/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 5 cars, 2 buss, 1 truck, 3 traffic lights, 1 backpack, 397.7ms\n",
      "video 1/1 (frame 130/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 2 buss, 3 traffic lights, 1 backpack, 386.8ms\n",
      "video 1/1 (frame 131/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 382.6ms\n",
      "video 1/1 (frame 132/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 389.1ms\n",
      "video 1/1 (frame 133/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 4 cars, 1 bus, 1 truck, 4 traffic lights, 2 backpacks, 384.9ms\n",
      "video 1/1 (frame 134/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 2 backpacks, 381.7ms\n",
      "video 1/1 (frame 135/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 7 persons, 3 cars, 1 motorcycle, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 391.5ms\n",
      "video 1/1 (frame 136/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 380.3ms\n",
      "video 1/1 (frame 137/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 395.8ms\n",
      "video 1/1 (frame 138/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 378.2ms\n",
      "video 1/1 (frame 139/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 386.0ms\n",
      "video 1/1 (frame 140/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 8 persons, 5 cars, 2 buss, 3 traffic lights, 1 backpack, 1 handbag, 376.0ms\n",
      "video 1/1 (frame 141/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 9 persons, 4 cars, 2 buss, 4 traffic lights, 2 backpacks, 1 handbag, 385.8ms\n",
      "video 1/1 (frame 142/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 3 cars, 2 buss, 5 traffic lights, 1 backpack, 2 handbags, 390.0ms\n",
      "video 1/1 (frame 143/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 2 cars, 2 buss, 4 traffic lights, 1 backpack, 1 handbag, 394.9ms\n",
      "video 1/1 (frame 144/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 3 cars, 2 buss, 4 traffic lights, 3 backpacks, 2 handbags, 389.2ms\n",
      "video 1/1 (frame 145/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 2 cars, 2 buss, 4 traffic lights, 3 backpacks, 1 handbag, 382.8ms\n",
      "video 1/1 (frame 146/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 1 car, 2 buss, 1 truck, 3 traffic lights, 2 backpacks, 2 handbags, 384.2ms\n",
      "video 1/1 (frame 147/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 2 buss, 1 truck, 3 traffic lights, 2 backpacks, 1 handbag, 382.4ms\n",
      "video 1/1 (frame 148/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 3 backpacks, 383.2ms\n",
      "video 1/1 (frame 149/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 1 bus, 1 truck, 4 traffic lights, 3 backpacks, 372.3ms\n",
      "video 1/1 (frame 150/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 1 bus, 3 traffic lights, 3 backpacks, 374.7ms\n",
      "video 1/1 (frame 151/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 6 cars, 1 bus, 3 traffic lights, 2 backpacks, 380.1ms\n",
      "video 1/1 (frame 152/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 6 cars, 1 bus, 1 truck, 3 traffic lights, 3 backpacks, 377.9ms\n",
      "video 1/1 (frame 153/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 3 backpacks, 376.7ms\n",
      "video 1/1 (frame 154/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 6 cars, 1 bus, 1 truck, 4 traffic lights, 4 backpacks, 3 handbags, 386.2ms\n",
      "video 1/1 (frame 155/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 4 cars, 1 bus, 1 truck, 4 traffic lights, 4 backpacks, 390.1ms\n",
      "video 1/1 (frame 156/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 3 backpacks, 382.7ms\n",
      "video 1/1 (frame 157/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 4 backpacks, 1 handbag, 396.8ms\n",
      "video 1/1 (frame 158/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 4 backpacks, 1 handbag, 1 book, 406.7ms\n",
      "video 1/1 (frame 159/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 3 buss, 4 traffic lights, 3 backpacks, 1 handbag, 373.2ms\n",
      "video 1/1 (frame 160/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 3 cars, 2 buss, 1 truck, 3 traffic lights, 4 backpacks, 371.3ms\n",
      "video 1/1 (frame 161/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 15 persons, 2 cars, 2 buss, 3 traffic lights, 3 backpacks, 1 handbag, 381.5ms\n",
      "video 1/1 (frame 162/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 3 cars, 2 buss, 3 traffic lights, 3 backpacks, 1 handbag, 383.3ms\n",
      "video 1/1 (frame 163/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 5 cars, 2 buss, 3 traffic lights, 4 backpacks, 1 handbag, 369.6ms\n",
      "video 1/1 (frame 164/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 5 cars, 2 buss, 3 traffic lights, 1 backpack, 1 handbag, 378.0ms\n",
      "video 1/1 (frame 165/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 5 cars, 2 buss, 1 truck, 3 traffic lights, 2 backpacks, 1 handbag, 386.8ms\n",
      "video 1/1 (frame 166/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 2 backpacks, 2 handbags, 371.0ms\n",
      "video 1/1 (frame 167/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 2 cars, 2 buss, 1 truck, 3 traffic lights, 1 backpack, 2 handbags, 381.5ms\n",
      "video 1/1 (frame 168/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 15 persons, 4 cars, 2 buss, 1 truck, 4 traffic lights, 2 backpacks, 2 handbags, 371.5ms\n",
      "video 1/1 (frame 169/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 6 cars, 1 truck, 4 traffic lights, 3 backpacks, 3 handbags, 398.5ms\n",
      "video 1/1 (frame 170/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 2 buss, 1 truck, 3 traffic lights, 1 backpack, 4 handbags, 401.2ms\n",
      "video 1/1 (frame 171/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 5 cars, 2 buss, 3 traffic lights, 3 backpacks, 2 handbags, 366.2ms\n",
      "video 1/1 (frame 172/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 2 handbags, 385.1ms\n",
      "video 1/1 (frame 173/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 5 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 1 handbag, 377.3ms\n",
      "video 1/1 (frame 174/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 15 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 2 handbags, 378.5ms\n",
      "video 1/1 (frame 175/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 2 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 2 handbags, 381.4ms\n",
      "video 1/1 (frame 176/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 1 handbag, 377.6ms\n",
      "video 1/1 (frame 177/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 6 cars, 1 truck, 3 traffic lights, 2 backpacks, 1 handbag, 382.1ms\n",
      "video 1/1 (frame 178/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 3 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 1 handbag, 382.2ms\n",
      "video 1/1 (frame 179/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 1 handbag, 383.4ms\n",
      "video 1/1 (frame 180/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 1 bus, 1 truck, 3 traffic lights, 2 backpacks, 3 handbags, 381.5ms\n",
      "video 1/1 (frame 181/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 2 backpacks, 2 handbags, 375.9ms\n",
      "video 1/1 (frame 182/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 2 backpacks, 4 handbags, 374.4ms\n",
      "video 1/1 (frame 183/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 3 cars, 1 bus, 1 truck, 4 traffic lights, 1 backpack, 3 handbags, 386.2ms\n",
      "video 1/1 (frame 184/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 6 cars, 1 bus, 1 truck, 4 traffic lights, 2 backpacks, 3 handbags, 380.7ms\n",
      "video 1/1 (frame 185/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 7 cars, 1 bus, 1 truck, 3 traffic lights, 1 backpack, 3 handbags, 366.2ms\n",
      "video 1/1 (frame 186/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 4 cars, 1 bus, 1 truck, 5 traffic lights, 1 backpack, 3 handbags, 381.9ms\n",
      "video 1/1 (frame 187/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 5 cars, 1 bus, 1 truck, 4 traffic lights, 3 handbags, 377.0ms\n",
      "video 1/1 (frame 188/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 14 persons, 4 cars, 2 buss, 1 truck, 4 traffic lights, 2 backpacks, 4 handbags, 375.7ms\n",
      "video 1/1 (frame 189/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 2 buss, 4 traffic lights, 3 backpacks, 4 handbags, 379.4ms\n",
      "video 1/1 (frame 190/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 1 bus, 4 traffic lights, 3 backpacks, 1 handbag, 1 suitcase, 366.4ms\n",
      "video 1/1 (frame 191/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 1 bus, 4 traffic lights, 3 backpacks, 1 handbag, 1 suitcase, 375.7ms\n",
      "video 1/1 (frame 192/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 2 cars, 2 buss, 4 traffic lights, 2 backpacks, 1 handbag, 1 suitcase, 381.3ms\n",
      "video 1/1 (frame 193/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 2 buss, 5 traffic lights, 1 backpack, 1 handbag, 1 suitcase, 371.5ms\n",
      "video 1/1 (frame 194/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 12 persons, 2 cars, 2 buss, 4 traffic lights, 1 backpack, 2 handbags, 374.2ms\n",
      "video 1/1 (frame 195/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 13 persons, 5 cars, 2 buss, 4 traffic lights, 1 backpack, 1 handbag, 387.3ms\n",
      "video 1/1 (frame 196/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 11 persons, 4 cars, 1 bus, 4 traffic lights, 2 backpacks, 2 handbags, 1 suitcase, 369.6ms\n",
      "video 1/1 (frame 197/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 5 cars, 1 bus, 4 traffic lights, 1 backpack, 1 handbag, 374.7ms\n",
      "video 1/1 (frame 198/198) u:\\home\\nithin005\\yolov9\\test_videos\\3.mp4: 448x640 10 persons, 5 cars, 1 bus, 3 traffic lights, 1 backpack, 376.4ms\n",
      "Speed: 1.8ms preprocess, 391.3ms inference, 11.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# # Build a YOLOv9c model from pretrained\n",
    "\n",
    "model = YOLO('yolov9c.pt')\n",
    "\n",
    "# # Run inference \n",
    "# #results = model('a.jpg', save=True)\n",
    "results = model('test_videos/3.mp4', save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d44fc9-9e3e-4336-a64f-b13aa318661d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# # Load an official or custom model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov9c.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# # Perform tracking with the model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#  # Tracking with default tracker\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# #results = model.track(source=\"1.mp4\", show=True)  # Tracking with ByteTrack tracker\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# #results = model.track(source=\"test_videos/1.mp4\", save=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrack(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_videos/3.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m,stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:724\u001b[0m, in \u001b[0;36mModel._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply to(), cpu(), cuda(), half(), float() to model tensors that are not parameters or registered buffers.\"\"\"\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_is_pytorch_model()\n\u001b[1;32m--> 724\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# reset predictor as device may have changed\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# was str(self.device) i.e. device(type='cuda', index=0) -> 'cuda:0'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:232\u001b[0m, in \u001b[0;36mBaseModel._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    Applies a function to all the tensors in the model that are not parameters or registered buffers.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m        (BaseModel): An updated BaseModel object.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Detect):  \u001b[38;5;66;03m# includes all Detect subclasses like Segment, Pose, OBB, WorldDetect\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mclea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    " from ultralytics import YOLO\n",
    "\n",
    "# # Load an official or custom model\n",
    " model = YOLO('yolov9c.pt').to('cuda')\n",
    "\n",
    "# # Perform tracking with the model\n",
    "#  # Tracking with default tracker\n",
    "# #results = model.track(source=\"1.mp4\", show=True)  # Tracking with ByteTrack tracker\n",
    "# #results = model.track(source=\"test_videos/1.mp4\", save=True)\n",
    " results = model.track(source=\"test_videos/3.mp4\",stream=True, save=True)\n",
    "\n",
    " for r in results:\n",
    "     boxes = r.boxes\n",
    "     masks = r.masks\n",
    "     probs = r.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2102690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643e378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5aa58f-04c8-475a-b976-c616897b01ba",
   "metadata": {},
   "source": [
    "# Train on custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e277a-18d8-4289-bb87-9d4473b508ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.2 🚀 Python-3.11.8 torch-2.2.2+cu121 CUDA:0 (NVIDIA GeForce RTX 4050 Laptop GPU, 6140MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov9c.pt, data=data.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train6\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    212864  ultralytics.nn.modules.block.RepNCSPELAN4    [128, 256, 128, 64, 1]        \n",
      "  3                  -1  1    164352  ultralytics.nn.modules.block.ADown           [256, 256]                    \n",
      "  4                  -1  1    847616  ultralytics.nn.modules.block.RepNCSPELAN4    [256, 512, 256, 128, 1]       \n",
      "  5                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      "  6                  -1  1   2857472  ultralytics.nn.modules.block.RepNCSPELAN4    [512, 512, 512, 256, 1]       \n",
      "  7                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      "  8                  -1  1   2857472  ultralytics.nn.modules.block.RepNCSPELAN4    [512, 512, 512, 256, 1]       \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPELAN         [512, 512, 256]               \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1   3119616  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 512, 512, 256, 1]      \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    912640  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 256, 256, 128, 1]      \n",
      " 16                  -1  1    164352  ultralytics.nn.modules.block.ADown           [256, 256]                    \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1   2988544  ultralytics.nn.modules.block.RepNCSPELAN4    [768, 512, 512, 256, 1]       \n",
      " 19                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   3119616  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 512, 512, 256, 1]      \n",
      " 22        [15, 18, 21]  1   5584342  ultralytics.nn.modules.head.Detect           [2, [256, 512, 512]]          \n",
      "YOLOv9c summary: 618 layers, 25530774 parameters, 25530758 gradients, 103.7 GFLOPs\n",
      "\n",
      "Transferred 931/937 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train6', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/nithin005/yolov9/custom_dataset/train/labels... 765 images, 0 backgrounds, 0 corrupt: 100%|██████████| 765/765 [00:00<00:00, 936.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/nithin005/yolov9/custom_dataset/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/nithin005/yolov9/custom_dataset/train/labels.cache... 765 images, 0 backgrounds, 0 corrupt: 100%|██████████| 765/765 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train6/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 154 weight(decay=0.0), 161 weight(decay=0.0005), 160 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train6\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /home/nithin005/.local/lib/python3.10/site-packages/nvidia/cudnn/lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERKNS1_20NormForwardOperationEmb, version libcudnn_cnn_infer.so.8\n",
      "  0%|          | 0/48 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GET was unable to find an engine to execute this computation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov9c.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# # Train the model for 100 epochs\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/ultralytics/engine/model.py:673\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/ultralytics/engine/trainer.py:199\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/ultralytics/engine/trainer.py:379\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    375\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GET was unable to find an engine to execute this computation"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# #https://universe.roboflow.com/mahmoud-9wyf6/fire-n-smoke-detection/dataset/1/download/yolov8\n",
    "\n",
    "# # Build a YOLOv9c model from pretrained weight\n",
    "model = YOLO('yolov9c.pt')\n",
    "\n",
    "# # Train the model for 100 epochs\n",
    "results = model.train(data='data.yaml', epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166c9c6-b1e8-41a1-b16a-729ed6de9d45",
   "metadata": {},
   "source": [
    "# Run inference on the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a0222-6541-43d4-95b9-1630aeb9abd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs/detect/train2/weights/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# # Load a pretrained YOLOv8n model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mruns/detect/train2/weights/best.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # Run inference on 'bus.jpg' with arguments\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_videos/fire.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/ultralytics/models/yolo/model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/ultralytics/engine/model.py:151\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/ultralytics/engine/model.py:240\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    237\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/ultralytics/nn/tasks.py:806\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/ultralytics/nn/tasks.py:732\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m temporary_modules(\n\u001b[1;32m    726\u001b[0m         {\n\u001b[1;32m    727\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.yolo.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    730\u001b[0m         }\n\u001b[1;32m    731\u001b[0m     ):  \u001b[38;5;66;03m# for legacy 8.0 Classify and Pose models\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m         ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/py3.11/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/detect/train2/weights/best.pt'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# # Load a pretrained YOLOv8n model\n",
    "model = YOLO('runs/detect/train2/weights/best.pt')\n",
    "\n",
    "# # Run inference on 'bus.jpg' with arguments\n",
    "model.predict('test_videos/fire.mp4', save=True, imgsz=640, conf=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
